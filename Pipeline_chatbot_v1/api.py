"""
api.py

FastAPI app serving a retrieval-augmented chatbot:
- Loads a Chroma vector DB and other components on startup.
- Provides a /chat endpoint that either asks clarifying questions or
    returns an LLM-generated answer with sources and image URLs.

Comments added to explain purpose of key functions and flow.
"""
import os
import traceback
from typing import List, Optional, Union, Tuple, Dict
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import uvicorn
from rag.build_chroma import build_or_load_chroma
from rag.retrieval import (
        make_retriever,
        return_top_doc,
        get_related_image_docs,
        build_image_notes,
        entity_pinned_candidates,
)
from rag.prompt import Chatbot_prompt
from rag.memory import HybridMemory
from rag.clarify import Clarifier, ClarifyState
from rag.focus import rewrite_followup_to_standalone
from tracking.local_tracker import get_tracker
from tracking.callbacks import UsageTrackingCallback

try:
    from langchain_chroma import Chroma
except ImportError:
    from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaLLM
from langchain.schema import Document

# Sentence transformer for topic embeddings used by memory
from sentence_transformers import SentenceTransformer

# Local config values (paths, model names, flags)
from config import (
        DOC_FOLDER,
        TOPIC_EMBEDDING_MODEL,
        USE_CUDA,
        URL,
        LLM_Model,
        LLM_TEMPERATURE,
        CLARIFY_MAX_TURNS,
        Image_format,
)

# Text LLM used for generating final answers (Ollama wrapper)
# Add usage tracking callback
usage_callback = UsageTrackingCallback(model_name=LLM_Model)
text_llm = OllamaLLM(
    model=LLM_Model,
    temperature=LLM_TEMPERATURE,
    callbacks=[usage_callback]
)
clarifier = Clarifier()  # clarifier utility (decides when to ask followups)

# FastAPI app setup
app = FastAPI(title="Chatbot")
vectordb = None
retriever = None
memory = None
clarify_state = None
MAX_LEN = 200

# Allow all origins for simplicity (change in production)
app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
)

# Serve docs/images/static files from DOC_FOLDER
app.mount("/static", StaticFiles(directory=DOC_FOLDER), name="static")

# Serve the frontend app directory (index.html etc.)
WEB_DIR = os.path.join(os.path.dirname(__file__), "web")
app.mount("/app", StaticFiles(directory=WEB_DIR), name="app")


@app.get("/")
def root():
        """
        Root: serve web index if present, otherwise redirect to OpenAPI docs.
        """
        index_path = os.path.join(WEB_DIR, "index.html")
        if os.path.exists(index_path):
                return FileResponse(index_path)
        return RedirectResponse(url="/docs")


@app.get("/favicon.ico")
def favicon():
        """
        Serve favicon (using scaner.png as fallback).
        """
        favicon_path = os.path.join(WEB_DIR, "scaner.png")
        if os.path.exists(favicon_path):
                return FileResponse(favicon_path)
        return RedirectResponse(url="/docs")


#################
def answer_or_clarify(
        user_q: str,
        retriever,
        vectordb: Chroma,
        memory: HybridMemory,
        cstate: ClarifyState,
        k_base: int = 60,
        k_entity: int = 24,
) -> Tuple[Dict[str, str], List[Document], List[Document]]:
        """
        Core logic that either:
        - asks a clarifying question (if docs are ambiguous), or
        - returns a final answer generated by the LLM with supporting docs and images.

        Steps:
        1. Compact history and get focus terms from memory.
        2. Rewrite follow-ups to standalone queries using focus terms & history.
        3. Retrieve candidate docs (entity-pinned retrieval if an entity is present).
        4. Rank & pick top docs, build context string.
        5. Find related image docs and build image notes.
        6. Decide whether to ask a clarifying question using Clarifier.
             - If clarify is required, return clarify payload and keep cstate updated.
        7. Recall relevant memory snippets and build memory_context.
        8. Construct prompt and call the LLM to generate an answer.
        9. Save the Q/A to memory and reset clarify state.
        """
        # Compact the chat history into a short context and possibly reset topic
        compact_history = memory.reset_if_new_topic(user_q)

        # Focus terms (optional topic/entity hints stored in memory)
        focus_terms = memory.get_focus()
        entity = focus_terms[0] if focus_terms else None

        # Rewrite follow-up questions into standalone queries using focus/history
        rewritten_q = rewrite_followup_to_standalone(user_q, focus_terms, compact_history)

        # Retrieval: combine base and entity-specific candidates
        candidates = entity_pinned_candidates(
                vectordb, retriever, rewritten_q, entity, k_base=k_base, k_entity=k_entity
        )

        # Rank and select top docs to use as context
        docs, scores = return_top_doc(rewritten_q, candidates, top_n=6)
        context = "\n\n".join([d.page_content for d in docs]) if docs else ""

        # Images related to query (for front-end display)
        image_docs = get_related_image_docs(vectordb, rewritten_q, k=2)
        image_notes = build_image_notes(image_docs)

        # Clarification logic: only ask up to CLARIFY_MAX_TURNS
        can_clarify = not cstate.active or cstate.turns_done < CLARIFY_MAX_TURNS
        needs_clarification = clarifier.needs_clarification(docs, scores)
        if can_clarify and needs_clarification:
                cstate.active = True
                if cstate.turns_done == 0:
                        cstate.original_question = rewritten_q
                c_q = clarifier.make_clarifying_question(rewritten_q, docs)
                cstate.last_bot_question = c_q
                cstate.turns_done += 1
                # Return early with clarify payload (frontend should prompt user)
                return ({"clarify": True, "clarify_question": c_q}, docs, image_docs)

        # Memory recall: retrieve relevant past snippets for context
        recalled = memory.recall(rewritten_q)
        memory_context = "\n".join([compact_history, "[Relevant Past]", *recalled]) if recalled else compact_history

        # Final answer: build prompt with context, images, memory, focus terms
        focus_terms_p = ", ".join(focus_terms) if focus_terms else "None"
        prompt = Chatbot_prompt.format(
                context=context,
                image_notes=image_notes,
                memory_context=memory_context,
                question=rewritten_q,
                focus_terms=focus_terms_p,
        )

        # Invoke the text LLM and append the Q/A to memory
        answer = text_llm.invoke(prompt).strip()
        memory.append(user_q, answer)

        # Reset clarify state after successful answer
        cstate.active = False
        cstate.turns_done = 0
        cstate.original_question = ""
        cstate.last_bot_question = ""

        return ({"clarify": False, "answer": answer}, docs, image_docs)


################

def like_image(path: str) -> bool:
        """
        Simple check whether a file extension matches known image formats.
        """
        _, ext = os.path.splitext(path or "")
        return ext.lower() in Image_format


def _rel_path_under_doc(abs_path: str) -> Union[str, None]:
        """
        Return a POSIX-style relative path under DOC_FOLDER, or None if the absolute
        path is outside the static root. This prevents path traversal to serve files
        outside the intended folder.
        """
        if not abs_path:
                return None
        root = os.path.abspath(DOC_FOLDER)
        ap = os.path.abspath(abs_path)
        try:
                rel = os.path.relpath(ap, root)
        except ValueError:
                return None
        if rel.startswith(".."):
                return None
        # Ensure forward slashes for URLs
        return rel.replace("\\", "/")


def _absolute_static_url(rel: str, request: Request) -> str:
        """
        Build an absolute URL to a static resource.
        If a global URL override is provided (e.g. deployed behind proxy), use it.
        Otherwise use FastAPI's url_for to construct the URL.
        """
        if URL:
                return f"{URL.rstrip('/')}/static/{rel.lstrip('/')}"
        return str(request.url_for("static", path=rel))


def path_to_static_url(abs_path: str, request: Request) -> Union[str, None]:
        """
        Convert an absolute filesystem path (under DOC_FOLDER) into a fully
        resolvable /static URL for the frontend. Returns None if path is outside DOC_FOLDER.
        """
        rel = _rel_path_under_doc(abs_path)
        if rel is None:
                return None
        return _absolute_static_url(rel, request)


def image_absolute_path(d) -> Union[str, None]:
        """
        Resolve an absolute image path from a Document's metadata:
        - Prefer metadata['image_path'] if present.
        - If relative, anchor it to the directory of metadata['source'] (the doc's file),
            otherwise anchor to DOC_FOLDER.
        - If no image_path, but source itself looks like an image, use that.
        Returns absolute filesystem path or None.
        """
        p = d.metadata.get("image_path", "")
        if not p:
                candidate = d.metadata.get("source", "")
                if like_image(candidate):
                        p = candidate

        if not p:
                return None

        if os.path.isabs(p):
                return p

        anchor = d.metadata.get("source", "")
        if anchor:
                base_dir = os.path.dirname(anchor)
        else:
                base_dir = DOC_FOLDER
        return os.path.abspath(os.path.join(base_dir, p))


# Request/response Pydantic models for /chat endpoint
class ChatRequest(BaseModel):
        message: str
        session_id: Optional[str] = None
        clarify_reply: Optional[bool] = False
        forced_version: Optional[str] = None


class Source(BaseModel):
        source: str


class ImageSource(BaseModel):
        path: str
        url: str
        caption: str


class ChatResponse(BaseModel):
        clarify: bool
        bot: str
        sources: List[Source] = []
        images: List[ImageSource] = []


def pack_sources(docs):
        """
        Convert Documents into a deduplicated list of Source models.
        """
        seen, out = set(), []
        for d in docs or []:
                src = d.metadata.get("source", "unknown")
                if src not in seen:
                        out.append(Source(source=src))
                        seen.add(src)
        return out


def pack_images(image_docs, request: Request):
        """
        Build ImageSource entries with absolute URLs so the frontend can render them.
        Skip anything outside DOC_FOLDER.
        """
        out = []
        for d in image_docs or []:
                abs_path = image_absolute_path(d)
                if not abs_path:
                        continue
                url = path_to_static_url(abs_path, request)
                if not url:
                        continue
                out.append(
                        ImageSource(
                                path=abs_path,
                                url=url,
                                caption=(d.page_content or "")[:MAX_LEN],
                        )
                )
        return out


@app.on_event("startup")
def startup():
        """
        Initialize global components on app start:
        - load/create Chroma vector DB
        - create a sentence-transformer for topic embeddings
        - build a retriever and memory object
        - initialize clarify state
        """
        global vectordb, retriever, memory, clarify_state
        vectordb = build_or_load_chroma()
        topic_embedder = SentenceTransformer(
                TOPIC_EMBEDDING_MODEL, device="cuda" if USE_CUDA else "cpu"
        )
        llm_for_memory = None
        retriever = make_retriever(vectordb)
        # HybridMemory stores short-term focus terms and longer recollections
        memory = HybridMemory(embedder=topic_embedder, llm=llm_for_memory or text_llm)
        clarify_state = ClarifyState()


@app.get("/metrics")
def metrics(
    hours: int = 24,
    model: Optional[str] = None,
    include_cumulative: bool = True,
    include_context_events: bool = True
):
    """
    Get LLM usage metrics and statistics.
    Query params:
    - hours: time window for stats (default: 24)
    - model: filter by specific model name (optional)
    - include_cumulative: include cumulative token counts across all time (default: True)
    - include_context_events: include context window clearing/reset events (default: True)
    """
    tracker = get_tracker()

    result = {
        "summary": tracker.get_stats(model_name=model, hours=hours),
        "model_comparison": tracker.get_model_comparison(hours=hours),
        "recent_calls": tracker.get_recent_calls(limit=10, model_name=model),
        "hourly_usage": tracker.get_hourly_usage(hours=hours)
    }

    # Add cumulative token consumption across all time
    if include_cumulative:
        result["cumulative"] = tracker.get_cumulative_tokens(model_name=model)

    # Add context window event tracking
    if include_context_events:
        result["context_events"] = {
            "stats": tracker.get_context_stats(hours=hours),
            "recent_events": tracker.get_context_events(hours=hours, limit=20)
        }

    return result


@app.post("/chat", response_model=ChatResponse)
def chat(chatrequest: ChatRequest, request: Request):
        """
        /chat endpoint:
        - If a clarify_reply is provided and a clarification is active, it appends the user's
            additional details and proceeds to answer.
        - Otherwise runs answer_or_clarify on the user's message.
        - If answer_or_clarify returns a clarify payload, respond with clarify=True so the frontend
            can ask the user for more details.
        - On success, return the bot answer, source list, and images (with absolute URLs).
        """
        try:
                if vectordb is None:
                        return ChatResponse(
                                clarify=False,
                                bot="No DB available. Please index documents.",
                                sources=[],
                                images=[],
                        )

                if chatrequest.clarify_reply and clarify_state and clarify_state.active:
                        # Compose a clarified question by attaching user's extra details
                        clarified_question = f"{clarify_state.original_question}\nAdditional details: {chatrequest.message}"
                        result, docs, image_docs = answer_or_clarify(
                                clarified_question, retriever, vectordb, memory, clarify_state
                        )
                else:
                        result, docs, image_docs = answer_or_clarify(
                                chatrequest.message, retriever, vectordb, memory, clarify_state
                        )

                if result.get("clarify"):
                        # The model decided a clarification is needed: send the follow-up question
                        return ChatResponse(
                                clarify=True,
                                bot=result["clarify_question"],
                                sources=[],
                                images=[],
                        )

                # Normal answer response
                return ChatResponse(
                        clarify=False,
                        bot=result.get("answer", "Sorry, I couldn't generate an answer."),
                        sources=pack_sources(docs),
                        images=pack_images(image_docs, request),
                )

        except Exception as e:
                traceback.print_exc()
                return ChatResponse(
                        clarify=False,
                        bot=f"Server error: {e}",
                        sources=[],
                        images=[],
                )


if __name__ == "__main__":
        # Run with reload during development
        uvicorn.run("api:app", host="0.0.0.0", port=8000, reload=True)
